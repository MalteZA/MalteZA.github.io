%%%% Guide til bibtex og litteratur.bib %%%%

% Alle kilder angives efter Harvard-metoden og vil i rapporten fremstaa med [FORFATTER, AAR] eller mere praecist [EFTERNAVN, AAR].

% Nederst findes eksempler paa kildeangivelser af typerne book, article, manual techreport og misc.
  % - Book bruges ved helt almindelige boeger
  % - Article bruges ved artikler og udgivelser
  % - Manual bruges ved henvisninger fra internettet, idet \url-funktionen findes her
  % - Techreport kan bruges ved mindre udgivelser, hvor kun titel, forfatter og aar er noedvendig eller tilgaengelig
  % - Misc kan bruges ved det, der falder udenfor de andre kategorier - interview, forelaesninger, foredrag osv.

% Hvis der er flere forfattere paa en kilde, SKAL de listes med 'and' imellem dem - ogsaa ved 3 og derover (f.eks. FORFATTER 1 and FORFATTER 2 and FORFATTER 3). Naar kildelisten bygges i rapporten, vil der ved 2 forfattere komme til at staa [EFTERNAVN 1 og EFTERNAVN 2, AAR] og ved 3 forfattere og derover komme til at staa [EFTERNAVN 1 et. al, AAR].

% Ved almindelige forfatternavne skal der vaere et saet tuborg-parenteser ved Author (f.eks. author = {H. C. Andersen}). Ved andre navne (f.eks. virksomheder, hjemmesider, grupper, raad, naevn osv.) skal der vaere 2 saet tuborg-parenteser ved Author (f.eks. author = {{Dansk Standard}}). Begrundelse: Med 2 saet tuborg-parenteser skrives hele parentesen ud som forfatter, [Dansk Standard, AAR]. Alternativt ville der i eksemplet bare komme til at staa [Standard, AAR].


@misc{minesweepers_facts,
	title = {Facts {About} {Landmines}},
	url = {https://landminefree.org/facts-about-landmines/},
	language = {en-GB},
	urldate = {2019-09-11},
	journal = {Minesweepers}
}

@misc{minesweepers_about,
	title = {About {Minesweepers}},
	url = {https://landminefree.org/facts-about-landmines/},
	language = {en-GB},
	urldate = {2019-09-11},
	journal = {Minesweepers}
}

@misc{LEGO_mindstorms_2013-1,
	title = {{MINDSTORMS} {EV}3 {User} {Guide}},
	url = {https://www.lego.com/cdn/cs/set/assets/bltbef4d6ce0f40363c/LMSUser_Guide_LEGO_MINDSTORMS_EV3_11_Tablet_ENUS.pdf},
	language = {en},
	urldate = {2019-09-24},
	author = {LEGO},
	year = {2013}
}

@misc{LEGO_lego_nodate,
	title = {{LEGO}® {MINDSTORMS}® {EV}3 31313 {\textbar} {MINDSTORMS}® {\textbar} {Buy} online at the {Official} {LEGO}® {Shop} {DK}},
	url = {https://www.lego.com/en-dk/product/lego-mindstorms-ev3-31313},
	abstract = {Create and command robots that do what you want with LEGO® MINDSTORMS® EV3!},
	language = {en},
	urldate = {2019-09-24}
}

@misc{LEGO_ev3_nodate,
	title = {{EV}3 {Cable} {Pack} 45514 {\textbar} {MINDSTORMS}® {\textbar} {Buy} online at the {Official} {LEGO}® {Shop} {DK}},
	url = {https://www.lego.com/en-dk/product/ev3-cable-pack-45514},
	language = {en},
	urldate = {2019-09-24}
}

@misc{LEGO_lego_nodate2,
	title = {{LEGO}® {MINDSTORMS}® {EV}3 31313 {\textbar} {MINDSTORMS}® {\textbar} {Buy} online at the {Official} {LEGO}® {Shop} {DK}},
	url = {https://www.lego.com/en-dk/product/lego-mindstorms-ev3-31313},
	abstract = {Create and command robots that do what you want with LEGO® MINDSTORMS® EV3!},
	language = {en},
	urldate = {2019-09-24}
}

@misc{lego_mindstorms_2009,
	title = {{MINDSTORMS} {NXT} 2.0 {User} {Guide}},
	url = {https://www.lego.com/biassets/bi/4589647.pdf},
	urldate = {2019-09-24},
	author = {{LEGO}},
	year = {2009}
}

@misc{raspberry_pi_foundation_raspberry_nodate,
	title = {Raspberry {Pi} {Foundation} - {About} {Us}},
	url = {https://www.raspberrypi.org/about/},
	abstract = {The Raspberry Pi Foundation is a UK-based charity that works to put the power of digital making into the hands of people all over the world.},
	language = {en-GB},
	urldate = {2019-09-24},
	author = {{Raspberry Pi Foundation}}
}

@misc{raspberry_pi_foundation_what_nodate,
	title = {What is a {Raspberry} {Pi}?},
	url = {https://www.raspberrypi.org/help/what-is-a-raspberry-pi/},
	abstract = {The Raspberry Pi is a low cost, credit-card sized computer that plugs into a computer monitor or TV, and uses a standard keyboard and mouse.},
	language = {en-GB},
	urldate = {2019-09-24},
	journal = {Raspberry Pi},
	author = {{Raspberry Pi Foundation}}
}

@misc{raspberry_pi_foundation_faqs_nodate,
	title = {{FAQs} - {Raspberry} {Pi} {Documentation}},
	url = {https://www.raspberrypi.org/documentation/faqs/},
	abstract = {Official documentation for the Raspberry Pi, written by the Raspberry Pi Foundation with community contributions.},
	urldate = {2019-09-24},
	author = {{Raspberry Pi Foundation}}
}

@misc{raspberry_pi_foundation_raspberry_nodate,
	title = {Raspberry {Pi} {Model} {A}},
	url = {https://www.raspberrypi.org/model-a/},
	abstract = {Model A is the lower-spec variant of the Raspberry Pi, with 256 MB of RAM, one USB port and no Ethernet port. Ideal for use as a media centre behind your TV.},
	language = {en-GB},
	urldate = {2019-10-04},
	journal = {Raspberry Pi},
	author = {{Raspberry Pi Foundation}}
}

@misc{raspberry_pi_foundation_bcm2835_nodate,
	title = {{BCM}2835 - {Raspberry} {Pi} {Documentation}},
	url = {https://www.raspberrypi.org/documentation/hardware/raspberrypi/bcm2835/README.md},
	abstract = {This section contains documentation with technical information about the Raspberry Pi hardware, including official add-ons and the Pi itself.},
	urldate = {2019-09-24},
	author = {{Raspberry Pi Foundation}}
}

@misc{arm_limited_arm11_nodate,
	title = {{ARM}11 processors},
	url = {http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.set.arm11/index.html&_ga=2.155964850.889934770.1569486481-1895601341.1569328253},
	urldate = {2019-09-24},
	author = {{Arm Limited}}
}

@article{raspberry_pi_foundation_raspberry_2019,
	title = {Raspberry {Pi} 4 specs and benchmarks},
	url = {https://www.raspberrypi.org/magpi/raspberry-pi-4-specs-benchmarks/},
	abstract = {Raspberry Pi 4 specs, specifications, and bechmarks from The MagPi, the official Raspberry Pi magazine},
	urldate = {2019-09-24},
	journal = {The MagPi Magazine},
	author = {{Raspberry Pi Foundation}},
	month = jun,
	year = {2019}
}

@incollection{montes_real-time_2019,
	address = {New York, NY},
	title = {Real-{Time} {Matlab}-{Simulink}-{Lego} {EV}3 {Framework} for {Teaching} {Robotics} {Subjects}},
	isbn = {9783319970844},
	abstract = {This work develops a new educational platform based on the Matlab-Simulink package for the teaching of robotics using the Lego EV3 platform. The majority of Lego platforms used in the literature are NXT platforms, as the EV3 platform is relatively new (January 2013). Moreover, in contrast to previous Lego robot versions, this platform allows to
develop a real-time framework to teach Robotics subjects. The framework is based in Matlab, the most widely used programming environment with LEGO Mindstorms, and employs the package provided by MathWorks. The proposed framework is tested here for a new motion
planning algorithm, where the user can interact with the environment and the robot in real-time via a web camera. To the authors’ knowledge, this is the first time a real-time application, capable to interact with the student, is developed to teach Robotics with EV3 platforms.},
	booktitle = {Robotics in education},
	publisher = {Springer Berlin Heidelberg},
	author = {Montés, Nicolás and Rosillo, Nuria and Covadonga Mora, Marta and Hilario, Lucia},
	year = {2019},
	pages = {230--240}
}
#https://link-springer-com.zorac.aub.aau.dk/book/10.1007%2F978-3-319-97085-1

@misc{arduino_arduino_nodate,
	title = {Arduino - {Introduction}},
	url = {https://www.arduino.cc/en/Guide/Introduction},
	urldate = {2019-09-30},
	author = {{Arduino}}
}

@misc{arduino_arduino_products,
	title = {Arduino - {Products}},
	url = {https://www.arduino.cc/en/Main/Products},
	urldate = {2019-09-30},
	author = {{Arduino}}
}

@misc{arduino_arduino_UNO,
	title = {Arduino {Uno} {Rev}3},
	url = {https://store.arduino.cc/usa/arduino-uno-rev3},
	abstract = {{\textless}p{\textgreater}{\textless}strong{\textgreater}Arduino Uno{\textless}/strong{\textgreater} is a microcontroller board based on the ATmega328P ({\textless}a href="http://ww1.microchip.com/downloads/en/DeviceDoc/Atmel-7810-Automotive-Microcontrollers-ATmega328P\_Datasheet.pdf" target="\_blank"{\textgreater}datasheet{\textless}/a{\textgreater}). It has 14 digital},
	urldate = {2019-09-30},
	author = {{Arduino}}
}

@misc{arduino_arduino_MEGA,
	title = {Arduino {Mega} 2560 {Rev}3},
	url = {https://store.arduino.cc/usa/mega-2560-r3},
	abstract = {{\textless}p{\textgreater}The {\textless}strong{\textgreater}Arduino Mega 2560{\textless}/strong{\textgreater} is a microcontroller board based on the\&nbsp;{\textless}a href="http://www.atmel.com/Images/Atmel-2549-8-bit-AVR-Microcontroller-ATmega640-1280-1281-2560-2561\_datasheet.pdf" target="\_blank"{\textgreater}ATmega2560{\textless}/a{\textgreater}. It has 54 digital},
	urldate = {2019-09-30},
	author = {{Arduino}}
}

@misc{BrickOWl-figure-EV3,
	url = {https://img.brickowl.com/files/image_cache/larger/lego-ev3-intelligent-brick-set-45500-15-1.jpg},
	urldate = {2019-09-30},
	author = {{Brick Owl}}
}

@misc{BrickOWl-figure-NXT2,
	url = {https://img.brickowl.com/files/image_cache/larger/lego-nxt-intelligent-brick-set-9841-15.jpg},
	urldate = {2019-09-30},
	author = {{Brick Owl}}
}

@misc{logitech_logitech_spec,
	title = {Logitech {HD} {Pro} {Webcam} {C}910 {Technical} {Specifications}},
	url = {http://support.logi.com/hc/en-in/articles/360023306114-Logitech-HD-Pro-Webcam-C910-Technical-Specifications},
	abstract = {NOTE: Information is for reference only and may be subject to change.



General Product Information:
[Compliance Certification (CE) Link]


Warranty / Self Help
Please see the product support page...},
	language = {en-IN},
	urldate = {2019-10-15},
	journal = {Logitech Support + Download},
	author = {{Logitech}}
}

@misc{RaspberryPI-figure-Pi4b,
	url = {https://www.raspberrypi.org/app/uploads/2019/06/HERO-ALT.jpg},
	urldate = {2019-09-24},
	author = {{Raspberry Pi Foundation}}
}

@misc{Arduino-figure-UNO,
	url = {https://store-cdn.arduino.cc/uni/catalog/product/cache/1/image/520x330/604a3538c15e081937dbfbd20aa60aad/a/0/a000066_featured_1_.jpg},
	urldate = {2019-09-30},
	author = {{Arduino}}
}

@misc{Arduino-figure-MEGA,
	url = {https://store-cdn.arduino.cc/uni/catalog/product/cache/1/image/520x330/604a3538c15e081937dbfbd20aa60aad/a/0/a000067_featured_1_.jpg},
	urldate = {2019-09-30},
	author = {{Arduino}}
}

@misc{BrickOWl-figure-EV3-largeServo,
	url = {https://img.brickowl.com/files/image_cache/larger/lego-ev3-large-servo-motor-set-45502-4.jpg},
	urldate = {2019-09-30},
	author = {{Brick Owl}}
}

@misc{BrickOWl-figure-EV3-mediumServo,
	url = {https://img.brickowl.com/files/image_cache/larger/lego-ev3-medium-servo-motor-set-45503-15.jpg},
	urldate = {2019-09-30},
	author = {{Brick Owl}}
}

@misc{BrickOWl-figure-NXT2-Servo,
	url = {https://img.brickowl.com/files/image_cache/larger/lego-interactive-servo-motor-set-9842-15.jpg},
	urldate = {2019-09-30},
	author = {{Brick Owl}}
}

@misc{BrickOWl-figure-Technic-Motor9v,
	url = {https://img.brickowl.com/files/image_cache/larger/lego-large-technic-motor-9v-2838-28.jpg},
	urldate = {2019-09-30},
	author = {{Brick Owl}}
}

@misc{BrickOWl-figure-Technic-MiniMotor9v,
	url = {https://img.brickowl.com/files/image_cache/larger/lego-small-technic-motor-42-grams-28.jpg},
	urldate = {2019-09-30},
	author = {{Brick Owl}}
}

@misc{lego_lego_EV3NXTCompatibility,
	title = {{LEGO}® {MINDSTORMS}® {EV}3 and {NXT} compatibility},
	url = {https://www.lego.com/en-gb/service/help/products/themes-sets/mindstorms/lego-mindstorms-ev3-and-nxt-compatibility-408100000007885},
	urldate = {2019-09-30},
	author = {{LEGO}}
}

@misc{hurbain_nxt_nodate,
	title = {{NXT}® motor internals},
	url = {http://www.philohome.com/nxtmotor/nxtmotor.htm},
	urldate = {2019-10-02},
	author = {Hurbain, Philippe }
}

@misc{hurbain_lego_technicmotorComp,
	title = {{LEGO} 9V {Technic} {Motors} compared characteristics},
	url = {http://www.philohome.com/motors/motorcomp.htm},
	urldate = {2019-10-02},
	author = {Hurbain, Philippe}
}

@book{gasperi_extreme_2009,
	address = {Berkeley, CA},
	title = {Extreme {NXT}: {Extending} the {LEGO} {MINDSTORMS} {NXT} to the {Next} {Level}},
	isbn = {9781430224532 9781430224549},
	shorttitle = {Extreme {NXT}},
	url = {http://link.springer.com/10.1007/978-1-4302-2454-9},
	language = {en},
	urldate = {2019-10-02},
	publisher = {Apress},
	author = {Gasperi, Michael and Hurbain, Philippe “Philo”},
	year = {2009},
	doi = {10.1007/978-1-4302-2454-9}
}

@misc{Logitech-figure-C910,
	url = {https://secure.logitech.com/assets/31671/c910-front-view.jpg},
	urldate = {2019-10-02},
	author = {{Logitech}}
}

@misc{lego_identifying_legoparts,
	title = {Identifying {LEGO}® set and part numbers},
	url = {https://www.lego.com/en-us/service/help/bricks-building/replacement-parts/identifying-lego-set-and-part-numbers-408100000009668},
	urldate = {2019-10-02},
	author = {{LEGO}}
}

@misc{digitalTrends_milestonesSelfdriving_2019,
	title = {10 {Major} {Milestones} in the {History} of {Self}-{Driving} {Cars}},
	url = {https://www.digitaltrends.com/cars/history-of-self-driving-cars-milestones/},
	abstract = {From a 1920s stunt in downtown New York to Google's Waymo, here are the events you should know about from the history of self-driving cars.},
	language = {en},
	urldate = {2019-09-19},
	journal = {Digital Trends},
	month = feb,
	year = {2019}
}

@misc{statista_roadAccidents_2019,
	title = {Contributing factors leading to road accidents in {Great} {Britain} 2017},
	url = {https://www.statista.com/statistics/323079/contributing-factors-leading-to-road-accidents-in-great-britain-uk/},
	abstract = {This statistic shows the percentage distribution of contributing factors leading to road accidents in Great Britain (UK) in 2017.},
	language = {en},
	urldate = {2019-09-19},
	journal = {Statista}
}

@article{thomas_identifying_2013,
	title = {Identifying the causes of road crashes in {Europe}},
	volume = {57},
	issn = {1943-2461},
	abstract = {This research applies a recently developed model of accident causation, developed to investigate industrial accidents, to a specially gathered sample of 997 crashes investigated in-depth in 6 countries. Based on the work of Hollnagel the model considers a collision to be a consequence of a breakdown in the interaction between road users, vehicles and the organisation of the traffic environment. 54\% of road users experienced interpretation errors while 44\% made observation errors and 37\% planning errors. In contrast to other studies only 11\% of drivers were identified as distracted and 8\% inattentive. There was remarkably little variation in these errors between the main road user types. The application of the model to future in-depth crash studies offers the opportunity to identify new measures to improve safety and to mitigate the social impact of collisions. Examples given include the potential value of co-driver advisory technologies to reduce observation errors and predictive technologies to avoid conflicting interactions between road users.},
	language = {eng},
	journal = {Annals of Advances in Automotive Medicine. Association for the Advancement of Automotive Medicine. Annual Scientific Conference},
	author = {Thomas, Pete and Morris, Andrew and Talbot, Rachel and Fagerlind, Helen},
	year = {2013},
	pmid = {24406942},
	pmcid = {PMC3861814},
	pages = {13--22}
}

@article{boudette_elon_2019,
	chapter = {Business},
	title = {Elon {Musk} {Predicts} {Tesla} {Driverless} {Taxi} {Fleet} {Next} {Year}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2019/04/22/business/elon-musk-tesla-autopilot.html},
	abstract = {Mr. Musk said Tesla cars would soon drive themselves and serve as robot taxis. But many experts think autonomous cars are at least several years away.},
	language = {en-US},
	urldate = {2019-09-19},
	journal = {The New York Times},
	author = {Boudette, Neal E.},
	month = apr,
	year = {2019},
	keywords = {Tesla Motors Inc, Musk, Elon, Driverless and Semiautonomous Vehicles, Traffic Accidents and Safety, Electric and Hybrid Vehicles, Company Reports, Stocks and Bonds, Taxicabs and Taxicab Drivers}
}


@misc{waymo_drivenTenBillionMiles,
	title = {Waymo has now driven 10 billion autonomous miles in simulation},
	url = {http://social.techcrunch.com/2019/07/10/waymo-has-now-driven-10-billion-autonomous-miles-in-simulation/},
	abstract = {Alphabet’s Waymo autonomous driving company announced a new milestone at TechCrunch Sessions: Mobility on Wednesday: 10 billion miles driving in simulation. This is a significant achievement for the company, because all those simulated miles on the road for its self-driving software add up to…},
	language = {en-US},
	urldate = {2019-09-19},
	journal = {TechCrunch}
}

@misc{waymo_homepage,
	title = {Waymo},
	url = {https://waymo.com/},
	abstract = {Waymo—formerly the Google self-driving car project—stands for a new way forward in mobility. Our mission is to make it safe and easy for people and things to move around.},
	language = {en},
	urldate = {2019-09-19},
	journal = {Waymo}
}

@inproceedings{wallace_roadFollowing_1986,
	address = {San Francisco, CA, USA},
	title = {Progress in robot road-following},
	volume = {3},
	url = {http://ieeexplore.ieee.org/document/1087503/},
	doi = {10.1109/ROBOT.1986.1087503},
	urldate = {2019-09-19},
	booktitle = {Proceedings. 1986 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	publisher = {Institute of Electrical and Electronics Engineers},
	author = {Wallace, R. and Matsuzaki, K. and Goto, Y. and Crisman, J. and Webb, J. and Kanade, T.},
	year = {1986},
	pages = {1615--1621}
}
@article{ukpata_traffic_2012,
	title = {Traffic {Congestion} in {Major} {Cities} of {Nigeria}},
	volume = {Volume 2 },
	issn = {2049-3444},
	language = {English},
	number = {No. 8},
	journal = {International Journal of Engineering and Technology },
	author = {Ukpata, Joseph. O.  and  Etika, Anderson A.},
	month = aug,
	year = {2012},
	pages = {1433 -- 1438}
}

@article{bull_urban_2002,
	title = {Urban traffic congestion: its economic and social causes and consequences},
	shorttitle = {Urban traffic congestion},
	url = {https://repositorio.cepal.org//handle/11362/10867},
	abstract = {Includes bibliography},
	language = {en},
	urldate = {2019-09-17},
	author = {Bull, Alberto and Thomson, Ian},
	month = apr,
	year = {2002}
}

@book{bull_traffic_2003,
	address = {Santiago, Chile},
	series = {Cuadernos de la {CEPAL}},
	title = {Traffic congestion: the problem and how to deal with it},
	isbn = {9789211214321},
	shorttitle = {Traffic congestion},
	number = {87},
	publisher = {United Nations, Economic Commission for Latin America and the Caribbean},
	editor = {Bull, Alberto and {United Nations} and {Deutsche Gesellschaft für Technische Zusammenarbeit}},
	year = {2003},
	note = {OCLC: ocm55119072},
	keywords = {Traffic congestion}
}

@misc{noauthor_waymo_driverless,
	title = {Waymo},
	url = {https://waymo.com/},
	abstract = {Waymo—formerly the Google self-driving car project—stands for a new way forward in mobility. Our mission is to make it safe and easy for people and things to move around.},
	language = {en},
	urldate = {2019-09-19},
	journal = {Waymo}
}

@article{daily_self-driving_2017,
	title = {Self-{Driving} {Cars}},
	volume = {50},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/8220479/},
	doi = {10.1109/MC.2017.4451204},
	number = {12},
	urldate = {2019-09-19},
	journal = {Computer},
	author = {Daily, Mike and Medasani, Swarup and Behringer, Reinhold and Trivedi, Mohan},
	month = dec,
	year = {2017},
	pages = {18--23}
}

@misc{vi.chilukuri.ctrdot.gov_usdot_2017,
	type = {Text},
	title = {{USDOT} {Releases} 2016 {Fatal} {Traffic} {Crash} {Data}},
	url = {https://www.nhtsa.gov/press-releases/usdot-releases-2016-fatal-traffic-crash-data},
	language = {en},
	urldate = {2019-09-19},
	journal = {NHTSA},
	author = {{vi.chilukuri.ctr@dot.gov}},
	month = oct,
	year = {2017}
}

@misc{matthew.lynberg.ctrdot.gov_automated_2017,
	type = {Text},
	title = {Automated {Vehicles} for {Safety}},
	url = {https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety},
	abstract = {The continuing evolution of automotive technology aims to deliver even greater safety benefits and Automated Driving Systems (ADS) that — one day — can handle the whole task of driving when we don’t want to or can’t do it ourselves. Fully automated cars and trucks that drive us, instead of us driving them, will become a reality. These self-driving vehicles ultimately will integrate onto U.S. roadways by progressing through six levels of driver assistance technology advancements in the coming years.},
	language = {en},
	urldate = {2019-09-19},
	journal = {NHTSA},
	author = {{matthew.lynberg.ctr@dot.gov}},
	month = sep,
	year = {2017}
}

@techreport{annual_accident_report_2018,
	title = {Annual {Accident} {Report} 2018},
	url = {https://ec.europa.eu/transport/road_safety/sites/roadsafety/files/pdf/statistics/dacota/asr2018.pdf},
	language = {English},
	author = {{National Technical University of Athens} and {Austrian Road Safety Board} and {European Union Road Federation}},
	urldate = {2019-09-19},
	month = may,
	year = {2018}
}

@misc{khan_road_2018,
	type = {Text},
	title = {Road {Safety} {Facts} \& {Figures}},
	url = {https://ec.europa.eu/transport/road_safety/road-safety-facts-figures-0_en},
	abstract = {Road Safety Facts \& Figures},
	language = {en},
	urldate = {2019-09-19},
	journal = {Mobility and transport - European Commission},
	author = {KHAN, Imran},
	month = may,
	year = {2018}
}

@book{danmark_why_accidents_happen_2014,
	title = {Why do road traffic accidents happen?},
	isbn = {978-87-91458-36-1},
	language = {English},
	publisher = {Danish Road Traffic Accident Investigation Board},
	author = {{Danmark} and {Havarikommissionen for Vejtrafikulykker}},
	year = {2014},
	note = {OCLC: 904518592}
}

@article{liu_infrastructure-review_2019,
	title = {A systematic review: {Road} infrastructure requirement for {Connected} and {Autonomous} {Vehicles} ({CAVs})},
	volume = {1187},
	issn = {1742-6588, 1742-6596},
	shorttitle = {A systematic review},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1187/4/042073},
	doi = {10.1088/1742-6596/1187/4/042073},
	number = {4},
	urldate = {2019-09-24},
	journal = {Journal of Physics: Conference Series},
	author = {Liu, Yuyan and Tight, Miles and Sun, Quanxin and Kang, Ruiyu},
	month = apr,
	year = {2019},
	pages = {042073}
}

@article{faulhaber_human_2019,
	title = {Human {Decisions} in {Moral} {Dilemmas} are {Largely} {Described} by {Utilitarianism}: {Virtual} {Car} {Driving} {Study} {Provides} {Guidelines} for {Autonomous} {Driving} {Vehicles}},
	volume = {25},
	issn = {1471-5546},
	shorttitle = {Human {Decisions} in {Moral} {Dilemmas} are {Largely} {Described} by {Utilitarianism}},
	url = {https://doi.org/10.1007/s11948-018-0020-x},
	doi = {10.1007/s11948-018-0020-x},
	abstract = {Ethical thought experiments such as the trolley dilemma have been investigated extensively in the past, showing that humans act in utilitarian ways, trying to cause as little overall damage as possible. These trolley dilemmas have gained renewed attention over the past few years, especially due to the necessity of implementing moral decisions in autonomous driving vehicles (ADVs). We conducted a set of experiments in which participants experienced modified trolley dilemmas as drivers in virtual reality environments. Participants had to make decisions between driving in one of two lanes where different obstacles came into view. Eventually, the participants had to decide which of the objects they would crash into. Obstacles included a variety of human-like avatars of different ages and group sizes. Furthermore, the influence of sidewalks as potential safe harbors and a condition implicating self-sacrifice were tested. Results showed that participants, in general, decided in a utilitarian manner, sparing the highest number of avatars possible with a limited influence by the other variables. Derived from these findings, which are in line with the utilitarian approach in moral decision making, it will be argued for an obligatory ethics setting implemented in ADVs.},
	language = {en},
	number = {2},
	urldate = {2019-09-24},
	journal = {Science and Engineering Ethics},
	author = {Faulhaber, Anja K. and Dittmer, Anke and Blind, Felix and Wächter, Maximilian A. and Timm, Silja and Sütfeld, Leon R. and Stephan, Achim and Pipa, Gordon and König, Peter},
	month = apr,
	year = {2019},
	keywords = {Autonomous driving ,  Utilitarianism ,  Trolley problem ,  Moral dilemma },
	pages = {399--418}
}

@misc{walker_av-regulation-us_2019,
	title = {Autonomous {Vehicle} {Regulations} – {Near}-{Term} {Challenges} and {Consequences}},
	url = {https://emerj.com/ai-sector-overviews/autonomous-vehicle-regulations-near-term/},
	abstract = {Several car makers predict they will able to make true self-driving cars in the next few years - as we've covered in our recent self-driving car timeline},
	language = {en-US},
	urldate = {2019-09-24},
	journal = {Emerj},
	author = {Walker, Jon}
}

@misc{autovista_av-regulation-eu_2019,
	title = {The state of autonomous legislation in {Europe} {\textbar} {Autovista} {Group}},
	url = {https://autovistagroup.com/news-and-insights/state-autonomous-legislation-europe},
	abstract = {28 February 2019

Manufacturers and technology companies across Europe are working to prepare autonomous vehicles for public use. Whether this is as part of a model launch or a mobility service, it is not a case of ‘if’ driverless cars will be seen on the roads, but ‘when’.},
	language = {en-US},
	urldate = {2019-09-24},
	journal = {Autovista Group},
	author = {Autovista Group}
}

@article{surden_technological_2016,
	title = {Technological {Opacity}, {Predictability}, and {Self}-{Driving} {Cars}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=2747491},
	doi = {10.2139/ssrn.2747491},
	language = {en},
	urldate = {2019-09-24},
	journal = {SSRN Electronic Journal},
	author = {Surden, Harry and Williams, Mary-Anne},
	year = {2016}
}

@misc{hao_three-tech-issues_2019,
	title = {The three challenges keeping cars from being fully autonomous},
	url = {https://www.technologyreview.com/s/613399/the-three-challenges-keeping-cars-from-being-fully-autonomous/},
	abstract = {Technical, regulatory, and business obstacles are still in the way of safe, useful, and affordable self-driving vehicles.},
	language = {en-US},
	urldate = {2019-09-24},
	journal = {MIT Technology Review},
	author = {Hao, Karen}
}

@misc{aberdeen_tech-issues_2018,
	title = {Tech and {Legal} {Challenges} the {Autonomous} {Car} {Industry} is {Facing}},
	url = {https://www.aberdeen.com/techpro-essentials/tech-legal-challenges-autonomous-car-industry-facing/},
	abstract = {The autonomous car industry is facing several technical as well as legal challenges, including radar interference, driving in extreme weather conditions, and the current lack of necessary laws and regulations.},
	language = {en-US},
	urldate = {2019-09-24},
	journal = {Aberdeen},
	author = {Contributor, Guest},
	month = may,
	year = {2018}
}

@article{martinez-diaz_av-prac-theory-issues_2018,
	title = {Autonomous vehicles: theoretical and practical challenges},
	volume = {33},
	issn = {23521465},
	shorttitle = {Autonomous vehicles},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352146518302606},
	doi = {10.1016/j.trpro.2018.10.103},
	language = {en},
	urldate = {2019-09-24},
	journal = {Transportation Research Procedia},
	author = {Martínez-Díaz, Margarita and Soriguera, Francesc},
	year = {2018},
	pages = {275--282}
}

@article{fagnant_preparing_2015,
	title = {Preparing a nation for autonomous vehicles: opportunities, barriers and policy recommendations},
	volume = {77},
	issn = {0965-8564},
	shorttitle = {Preparing a nation for autonomous vehicles},
	url = {http://www.sciencedirect.com/science/article/pii/S0965856415000804},
	doi = {10.1016/j.tra.2015.04.003},
	abstract = {Autonomous vehicles (AVs) represent a potentially disruptive yet beneficial change to our transportation system. This new technology has the potential to impact vehicle safety, congestion, and travel behavior. All told, major social AV impacts in the form of crash savings, travel time reduction, fuel efficiency and parking benefits are estimated to approach \$2000 to per year per AV, and may eventually approach nearly \$4000 when comprehensive crash costs are accounted for. Yet barriers to implementation and mass-market penetration remain. Initial costs will likely be unaffordable. Licensing and testing standards in the U.S. are being developed at the state level, rather than nationally, which may lead to inconsistencies across states. Liability details remain undefined, security concerns linger, and without new privacy standards, a default lack of privacy for personal travel may become the norm. The impacts and interactions with other components of the transportation system, as well as implementation details, remain uncertain. To address these concerns, the federal government should expand research in these areas and create a nationally recognized licensing framework for AVs, determining appropriate standards for liability, security, and data privacy.},
	urldate = {2019-09-24},
	journal = {Transportation Research Part A: Policy and Practice},
	author = {Fagnant, Daniel J. and Kockelman, Kara},
	month = jul,
	year = {2015},
	keywords = {Vehicle automation, Autonomous vehicles, Cost-benefit analysis, Safety, Congestion, Market penetration},
	pages = {167--181}
}

@misc{tesla_technology,
	title = {3 eller 30 år: {Hvornår} kommer selvkørende biler på vejene?},
	shorttitle = {3 eller 30 år},
	url = {https://www.dr.dk/nyheder/viden/tech/3-eller-30-aar-hvornaar-kommer-selvkoerende-biler-paa-vejene},
	abstract = {Teknologien er begyndt at bane vejen for en førerløs fremtid. Her er en række eksempler.},
	language = {da-DK},
	urldate = {2019-09-24},
	journal = {DR}
}

@misc{lanctot_accelerating_2017,
	title = {Accelerating the {Future}: {The} {Economic} {Impact} of the {Emerging} {Passenger} {Economy}},
	url = {https://www.strategyanalytics.com/access-services/automotive/autonomous-vehicles/reports/report-detail/accelerating-the-future-the-economic-impact-of-the-emerging-passenger-economy?slid=923371&spg=1},
	abstract = {Autonomous Vehicle Research and Analysis from Strategy Analytics.},
	language = {en},
	urldate = {2019-09-24},
	author = {Lanctot, Roger},
	month = jun,
	year = {2017},
}

@misc{self_autonomous_cars_finally_here,
	title = {Autonomous {Cars}},
	url = {https://www.theverge.com/autonomous-cars},
	abstract = {Self-driving cars are finally here, and how they are deployed will change how we get around forever. From Tesla to Google to Uber to all the major automakers, we bring you complete coverage of the race to develop fully autonomous vehicles. This includes helpful explanations about the technology and policies that underpin the movement to build driverless cars.},
	language = {en},
	urldate = {2019-09-26}
}

@misc{driverless_Cars_Are_Hot_topic,
	title = {Driverless cars: {Explained}},
	shorttitle = {Driverless cars},
	url = {http://a.msn.com/08/en-za/BBTqQ8j},
	abstract = {Driverless cars are a hot topic in the motoring world at the moment. Everyone’s talking about them; manufacturers, politicians and transport chiefs are all nattering on about the future of autonomous vehicles. But what are they, and what do they mean for the future of motoring? We’re going to try and clear things up.},
	language = {en-ZA},
	urldate = {2019-09-26},
	journal = {MSN}
}

@misc{Autonomous_future_,
	title = {The {Future} of {Autonomous} {Vehicles}},
	url = {https://www.futuresplatform.com/blog/future-autonomous-vehicles},
	abstract = {The autonomous vehicle is set to be as life-changing as the invention of the motor vehicle itself. Modern day cities have been shaped largely by the mobility that is achieved through motor vehicles, providing transport services to people and goods alike and supported by major road networks worldwide. Up until recently, the motor vehicle has been an extension of the human ambulatory system, docile to the drivers’ commands.},
	language = {en},
	urldate = {2019-09-26},
	journal = {Future Proof}
}

@article{yurtsever_survey_2019,
	title = {A {Survey} of {Autonomous} {Driving}: {Common} {Practices} and {Emerging} {Technologies}},
	shorttitle = {A {Survey} of {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1906.05113},
	abstract = {Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions: localization, mapping, perception, planning, and human machine interface, were thoroughly reviewed. Furthermore, the state-of-the-art was implemented on our own platform and various algorithms were compared in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.},
	urldate = {2019-09-27},
	journal = {arXiv:1906.05113 [cs, eess]},
	author = {Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.05113},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control}
}


@misc{university_of_toronto_self-driving_vehicle,
	title = {Lesson 1: {Sensors} and {Computing} {Hardware} - {Module} 2: {Self}-{Driving} {Hardware} and {Software} {Architectures}},
	shorttitle = {Lesson 1},
	url = {https://www.coursera.org/lecture/intro-self-driving-cars/lesson-1-sensors-and-computing-hardware-LrLty},
	abstract = {Video created by University of Toronto for the course "Introduction to Self-Driving Cars". System architectures for self-driving vehicles are extremely diverse, as no standardized solution has yet emerged.  This module describes both the hardware ...},
	language = {en},
	urldate = {2019-09-27},
	journal = {Coursera}
}

@inproceedings{zhang_sensor_2012,
	title = {A sensor fusion approach for localization with cumulative error elimination},
	doi = {10.1109/MFI.2012.6343009},
	abstract = {This paper describes a robust approach which improves the precision of vehicle localization in complex urban environments by fusing data from GPS, gyroscope and velocity sensors. In this method, we apply Kalman filter to estimate the position of the vehicle. Compared with other fusion based localization approaches, we process the data in a public coordinate system, called Earth Centred Earth Fixed (ECEF) coordinates and eliminate the cumulative error by its statistics characteristics. The contribution is that it not only provides a sensor fusion framework to estimate the position of the vehicle, but also gives a mathematical solution to eliminate the cumulative error stems from the relative pose measurements (provided by the gyroscope and velocity sensors). The experiments exhibit the reliability and the feasibility of our approach in large scale environment.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Multisensor} {Fusion} and {Integration} for {Intelligent} {Systems} ({MFI})},
	author = {Zhang, F. and Stähle, H. and Chen, G. and Simon, C. C. C. and Buckl, C. and Knoll, A.},
	month = sep,
	year = {2012},
	keywords = {Global Positioning System, gyroscopes, Kalman filters, position measurement, sensor fusion, velocity measurement, sensor fusion approach, cumulative error elimination, vehicle localization, GPS, gyroscope, velocity sensors, Kalman filter, public coordinate system, Earth centred Earth fixed coordinates, ECEF, pose measurements, Vehicles, Global Positioning System, Gyroscopes, Sensor fusion, Kalman filters, Estimation},
	pages = {1--6}
}

@misc{what_is_gnss,
	title = {What is {GNSS}?},
	url = {https://www.gsa.europa.eu/european-gnss/what-gnss},
	abstract = {Global Navigation Satellite System (GNSS) refers to a constellation of satellites providing signals from space that transmit positioning and timing data to GNSS receivers. The receivers then use this data to determine location.},
	language = {en},
	urldate = {2019-09-30},
	month = mar,
	year = {2016}
}

@misc{charles_pao_imu,
	title = {The importance of {IMU} {Motion} {Sensors}},
	url = {https://www.ceva-dsp.com/ourblog/what-is-an-imu-sensor/},
	abstract = {Weaving in between cars in a game on your phone, keeping a quadcopter drone afloat despite changing winds, vacuuming your house with a robot, and translating your precise movements when wearing a VR headset all},
	language = {en-US},
	urldate = {2019-09-30},
	journal = {CEVA’s Experts blog},
	month = nov,
	year = {2018}
}

@misc{objectDetection_yolo:_02-10,
	title = {{YOLO}: {Real}-{Time} {Object} {Detection}},
	url = {https://pjreddie.com/darknet/yolo/},
	abstract = {You only look once (YOLO) is a state-of-the-art, real-time object detection system.},
	urldate = {2019-10-02}
}

@misc{yurtsever_driving_2019,
	title = {Driving risk assessment with deep learning using a monocular camera. {Related} paper: https://arxiv.org/abs/1906.02859 : {Ekim}-{Yurtsever}/{DeepTL}-{Lane}-{Change}-{Classification}},
	copyright = {MIT},
	shorttitle = {Driving risk assessment with deep learning using a monocular camera. {Related} paper},
	url = {https://github.com/Ekim-Yurtsever/DeepTL-Lane-Change-Classification},
	urldate = {2019-10-02},
	author = {Yurtsever, Ekim},
	month = oct,
	year = {2019},
	note = {original-date: 2018-10-19T02:53:10Z}
}

@article{kocic_end--end_2019,
	title = {An {End}-to-{End} {Deep} {Neural} {Network} for {Autonomous} {Driving} {Designed} for {Embedded} {Automotive} {Platforms}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6539483/},
	doi = {10.3390/s19092064},
	abstract = {In this paper, one solution for an end-to-end deep neural network for autonomous driving is presented. The main objective of our work was to achieve autonomous driving with a light deep neural network suitable for deployment on embedded automotive platforms. There are several end-to-end deep neural networks used for autonomous driving, where the input to the machine learning algorithm are camera images and the output is the steering angle prediction, but those convolutional neural networks are significantly more complex than the network architecture we are proposing. The network architecture, computational complexity, and performance evaluation during autonomous driving using our network are compared with two other convolutional neural networks that we re-implemented with the aim to have an objective evaluation of the proposed network. The trained model of the proposed network is four times smaller than the PilotNet model and about 250 times smaller than AlexNet model. While complexity and size of the novel network are reduced in comparison to other models, which leads to lower latency and higher frame rate during inference, our network maintained the performance, achieving successful autonomous driving with similar efficiency compared to autonomous driving using two other models. Moreover, the proposed deep neural network downsized the needs for real-time inference hardware in terms of computational power, cost, and size.},
	number = {9},
	urldate = {2019-09-30},
	journal = {Sensors (Basel, Switzerland)},
	author = {Kocić, Jelena and Jovičić, Nenad and Drndarević, Vujo},
	month = may,
	year = {2019},
	pmid = {31058820},
	pmcid = {PMC6539483}
}

@article{pendleton_perception_2017,
	title = {Perception, {Planning}, {Control}, and {Coordination} for {Autonomous} {Vehicles}},
	volume = {5},
	issn = {2075-1702},
	url = {http://www.mdpi.com/2075-1702/5/1/6},
	doi = {10.3390/machines5010006},
	language = {en},
	number = {1},
	urldate = {2019-09-30},
	journal = {Machines},
	author = {Pendleton, Scott and Andersen, Hans and Du, Xinxin and Shen, Xiaotong and Meghjani, Malika and Eng, You and Rus, Daniela and Ang, Marcelo},
	month = feb,
	year = {2017},
	pages = {6}
}


@misc{6_levels_with_examples,
	title = {The 6 levels of self-driving car - and what they mean for motorists},
	url = {https://www.thejournal.ie/self-driving-cars-autonomy-levels-3603253-Sep2017/},
	abstract = {SAE has defined six levels of autonomy for self-driving, here’s what they mean.},
	language = {en},
	urldate = {2019-10-07},
	journal = {TheJournal.ie},
	author = {May, Melanie}
}

@misc{d2l_12-3-_10-10,
	title = {12.3. {Object} {Detection} and {Bounding} {Boxes} — {Dive} into {Deep} {Learning} 0.7 documentation},
	url = {http://d2l.ai/chapter_computer-vision/bounding-box.html},
	urldate = {2019-10-10}
}

@misc{mittal_instance_2019,
	title = {Instance segmentation using {Mask} {R}-{CNN}},
	url = {https://towardsdatascience.com/instance-segmentation-using-mask-r-cnn-7f77bdd46abd},
	abstract = {This article discusses the instance segmentation technique, Mask R-CNN, with a brief introduction to object detection techniques such as…},
	language = {en},
	urldate = {2019-10-10},
	journal = {Medium},
	author = {Mittal, Aditi},
	month = jun,
	year = {2019}
}

<<<<<<< HEAD
@misc{moog_car_steering_2019,
	title = {How a car steering system works: easy guide {\textbar} {MOOG}},
	shorttitle = {How a car steering system works},
	url = {https://www.moogparts.eu/blog/how-a-steering-system-works.html},
	abstract = {How do car steering systems work? Get to know the function of a steering system and its role in great handling with MOOG.},
	language = {en},
	urldate = {2019-10-30},
	journal = {MOOG Parts},
	author = {{MOOG}}
}

@misc{collins_car_steering_2018,
	title = {How {Car} {Steering} {Systems} {Work}},
	url = {https://www.carbibles.com/car-steering-systems/},
	abstract = {You may have a very powerful engine and a super-responsive, highly efficient transmission that transfers all this raw energy to your wheels, but if you don’t have a way to control the wheels and maneuver them in the direction you want to go, then you’d still end up with nothing less than a highly glorified …},
	language = {en},
	urldate = {2019-10-30},
	journal = {Carbibles},
	author = {Collins, Dan},
	month = jan,
	year = {2018}
}

@misc{gareffa_car_drive_2019,
	title = {All {About} {Front}-, {Rear}-, {Four}- and {All}-{Wheel} {Drive}},
	url = {https://www.edmunds.com/car-technology/what-wheel-drive.html},
	abstract = {How to choose between front-, rear-, four- and all-wheel-drive vehicles.},
	language = {en-us},
	urldate = {2019-10-30},
	journal = {Edmunds},
	author = {Gareffa, Peter}
}

@misc{glon_car_drive_2019,
	title = {{FWD} vs. {RWD} vs. {AWD}: {How} the wheels that turn change the way you drive},
	shorttitle = {{FWD} vs. {RWD} vs. {AWD}},
	url = {https://www.digitaltrends.com/cars/fwd-vs-awd-vs-rwd/},
	abstract = {Front-wheel drive (FWD), rear-wheel drive (RWD), and four- or all-wheel drive (4WD or AWD): you’ve probably heard these terms kicked around before. Here's the difference between the three, along with our description of the advantages and disadvantages of each drivetrain system.},
	language = {en},
	urldate = {2019-10-30},
	journal = {Digital Trends},
	author = {Glon, Ronan},
	month = sep,
	year = {2019}
}

@inproceedings{yang_end--end_2018,
	title = {End-to-end {Multi}-{Modal} {Multi}-{Task} {Vehicle} {Control} for {Self}-{Driving} {Cars} with {Visual} {Perceptions}},
	doi = {10.1109/ICPR.2018.8546189},
	abstract = {Convolutional Neural Networks (CNN) have been successfully applied to autonomous driving tasks, many in an end-to-end manner. Previous end-to-end steering control methods take an image or an image sequence as the input and directly predict the steering angle with CNN. Although single task learning on steering angles has reported good performances, the steering angle alone is not sufficient for vehicle control. In this work, we propose a multi-task learning framework to predict the steering angle and speed control simultaneously in an end-to-end manner. Since it is nontrivial to predict accurate speed values with only visual inputs, we first propose a network to predict discrete speed commands and steering angles with image sequences. Moreover, we propose a multi-modal multi-task network to predict speed values and steering angles by taking previous feedback speeds and visual recordings as inputs. Experiments are conducted on the public Udacity dataset and a newly collected SAIC dataset. Results show that the proposed model predicts steering angles and speed values accurately. Furthermore, we improve the failure data synthesis methods to solve the problem of error accumulation in real road tests.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Yang, Zhengyuan and Zhang, Yixuan and Yu, Jerry and Cai, Junjie and Luo, Jiebo},
	month = aug,
	year = {2018},
	note = {ISSN: 1051-4651},
	keywords = {automobiles, control system synthesis, feedback, feedforward neural nets, image sequences, learning (artificial intelligence), mobile robots, road vehicles, robot vision, steering systems, traffic engineering computing, velocity control, speed values, multimodal multitask vehicle control, autonomous driving tasks, end-to-end manner, previous end-to-end steering control methods, image sequence, steering angle, multitask learning framework, multimodal multitask network, self-driving cars, visual perceptions, convolutional neural networks, CNN, public Udacity dataset, SAIC dataset, failure data synthesis methods, road tests, Visualization, Roads, Cameras, Task analysis, Image sequences, Velocity control, Kernel},
	pages = {2289--2294}
}

@misc{patil_convnets_2019,
	title = {{ConvNets} \& {Driverless} {Cars}},
	url = {https://medium.com/datadriveninvestor/convnets-driverless-cars-997976c5ee4c},
	abstract = {Computers + Cars = Driverless cars. Whether or not you have you ever seen one, we are all fascinated by how this can be possible. After a…},
	language = {en},
	urldate = {2019-11-18},
	journal = {Medium},
	author = {Patil, Abhinit},
	month = mar,
	year = {2019}
}

@article{bojarski_end--end-nvidea_2016,
author = {Bojarski, Mariusz and Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Larry and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
year = {2016},
month = {04},
pages = {},
title = {End to End Learning for Self-Driving Cars}
}

@misc{nain_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {https://medium.com/@nainaakash012/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-92941c5bfb95},
	abstract = {Since AlexNet won the 2012 ImageNet competition, CNNs (short for Convolutional Neural Networks) have become the de facto algorithms for a…},
	language = {en},
	urldate = {2019-11-18},
	journal = {Medium},
	author = {Nain, Aakash},
	month = jun,
	year = {2019}
}

@article{zhang_yolo--v3_2018,
	title = {A {Fast} {Learning} {Method} for {Accurate} and {Robust} {Lane} {Detection} {Using} {Two}-{Stage} {Feature} {Extraction} with {YOLO} v3},
	volume = {18},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308794/},
	doi = {10.3390/s18124308},
	abstract = {To improve the accuracy of lane detection in complex scenarios, an adaptive lane feature learning algorithm which can automatically learn the features of a lane in various scenarios is proposed. First, a two-stage learning network based on the YOLO v3 (You Only Look Once, v3) is constructed. The structural parameters of the YOLO v3 algorithm are modified to make it more suitable for lane detection. To improve the training efficiency, a method for automatic generation of the lane label images in a simple scenario, which provides label data for the training of the first-stage network, is proposed. Then, an adaptive edge detection algorithm based on the Canny operator is used to relocate the lane detected by the first-stage model. Furthermore, the unrecognized lanes are shielded to avoid interference in subsequent model training. Then, the images processed by the above method are used as label data for the training of the second-stage model. The experiment was carried out on the KITTI and Caltech datasets, and the results showed that the accuracy and speed of the second-stage model reached a high level.},
	number = {12},
	urldate = {2019-11-18},
	journal = {Sensors (Basel, Switzerland)},
	author = {Zhang, Xiang and Yang, Wei and Tang, Xiaolin and Liu, Jie},
	month = dec,
	year = {2018},
	pmid = {30563274},
	pmcid = {PMC6308794}
}

@misc{sensorray_rgb--vs--yuv_2019,
	title = {Color {Spaces} in {Frame} {Grabbers}: {RGB} vs. {YUV}},
	url = {http://www.sensoray.com/support/appnotes/frame_grabber_capture_modes.htm},
	abstract = {Application Note: Color Spaces in Frame Grabbers.},
	urldate = {2019-11-18}
}

@misc{wikipeadia_yuv_2019,
	title = {{YUV}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=YUV&oldid=921160678},
	abstract = {YUV is a color encoding system typically used as part of a color image pipeline. It encodes a color image or video taking human perception into account, allowing reduced bandwidth for chrominance components, thereby typically enabling transmission errors or compression artifacts to be more efficiently masked by the human perception than using a "direct" RGB-representation. Other color encodings have similar properties, and the main reason to implement or investigate properties of Y′UV would be for interfacing with analog or digital television or photographic equipment that conforms to certain Y′UV standards.
The Y′UV model defines a color space in terms of one luma component (Y′) and two chrominance components, called U (blue projection) and V (red projection) respectively. The Y′UV color model is used in the PAL composite color video (excluding PAL-N) standard. Previous black-and-white systems used only luma (Y′) information.  Color information (U and V) was added separately via a subcarrier so that a black-and-white receiver would still be able to receive and display a color picture transmission in the receiver's native black-and-white format.
Y′ stands for the luma component (the brightness) and U and V are the chrominance (color) components; luminance is denoted by Y and luma by Y′ – the prime symbols (') denote gamma compression, with "luminance" meaning physical linear-space brightness, while "luma" is (nonlinear) perceptual brightness.
The scope of the terms Y′UV, YUV, YCbCr, YPbPr, etc., is sometimes ambiguous and overlapping. Historically, the terms YUV and Y′UV were used for a specific analog encoding of color information in television systems, while YCbCr was used for digital encoding of color information suited for video and still-image compression and transmission such as MPEG and JPEG. Today, the term YUV is commonly used in the computer industry to describe file-formats that are encoded using YCbCr.
The YPbPr color model used in analog component video and its digital version YCbCr used in digital video are more or less derived from it, and are sometimes called Y′UV. (CB/PB and CR/PR are deviations from grey on blue–yellow and red–cyan axes, whereas U and V are blue–luminance and red–luminance differences respectively.) The Y′IQ color space used in the analog NTSC television broadcasting system is related to it, although in a more complex way. The YDbDr color space used in the analog SECAM and PAL-N television broadcasting systems, are also related.
As for etymology, Y, Y′, U, and V are not abbreviations. The use of the letter Y for luminance can be traced back to the choice of XYZ primaries. This lends itself naturally to the usage of the same letter in luma (Y′), which approximates a perceptually uniform correlate of luminance. Likewise, U and V were chosen to differentiate the U and V axes from those in other spaces, such as the x and y chromaticity space. See the equations below or compare the historical development of the math.},
	language = {en},
	urldate = {2019-11-18},
	journal = {Wikipedia},
	month = oct,
	year = {2019},
	note = {Page Version ID: 921160678}
}
=======
@misc{TraxxasTRX4,
       title = {{TRX}-4® {Unassembled} {Kit}: 4WD {Chassis} with {TQi} {Traxxas} {Link}™ {Enabled} 2.4GHz {Radio} {System}},
       shorttitle = {{TRX}-4® {Unassembled} {Kit}},
       url = {https://traxxas.com/products/models/electric/trx-4-kit},
       urldate = {2019-10-30},
       month = feb,
       year = {2018}
}

@misc{JetsonNano,
       title = {{NVIDIA Jetson Nano Developer Kit}},
       url = {https://developer.nvidia.com/embedded/jetson-nano-developer-kit},
       urldate = {2019-10-30},
       month = mar,
       year = {2019}
}

@misc{WebcamSearch1,
       title = {{Best webcam 2019}},
       url = {https://www.techradar.com/news/computing-components/peripherals/what-webcam-5-reviewed-and-rated-1027972},
       urldate = {2019-10-30},
       month = oct,
       year = {2019}
}

@misc{WebcamSearch2,
       title = {{Best webcams 2019}},
       url = {https://www.expertreviews.co.uk/accessories/1406114/best-webcams},
       urldate = {2019-10-30},
       month = jun,
       year = {2019}
}

@misc{WebcamSearch3,
       title = {{Best webcams for 2019}},
       url = {https://www.pcgamer.com/best-webcams/},
       urldate = {2019-11-07},
       month = oct,
       year = {2019}
}

@misc{C922Spec,
       title = {{Logitech C922 Pro Stream | Specifications}},
       url = {https://www.logitech.com/da-dk/product/c922-pro-stream-webcam#specification-tabular},
       urldate = {2019-10-30},
       year = {2019}
}

@misc{JetPack,
       title = {{JetPack | NVIDIA Developer}},
       url = {https://developer.nvidia.com/embedded/jetpack},
       urldate = {2019-10-30},
       month = aug,
       year = {2019}
}
>>>>>>> 51c0b00a7cd8b4e72068d97f87e71110b9b807ba
